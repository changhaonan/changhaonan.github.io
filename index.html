<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haonan Chang</title>

  <meta name="author" content="Haonan Chang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Haonan Chang
                  </p>
                  <p>I'm a fifth year robotics Ph.D. stundet at Rutgers University in New Brunswick advised by Prof. Abdeslam Boularias. I am currently
                    working on combining latest generative model such as LLM, VLM, and VDM with robot manipulation to achieve robotics and general manipulation ability.
                  </p>
                  <p>
                    At Rutgers I've worked on LLM/Drive Manipulation: <a href="https://github.com/changhaonan/A3VLM">A3VLM</a>, <a href="https://lgmcts.github.io/">LGMCTS</a>; LLM-driven Scene
                    understanding: <a href="https://github.com/changhaonan/OVSG">OVSG</a>; Dynamic Scene Reconstruction: <a href="https://github.com/changhaonan/Mono-STAR-demo">Mono-STAR</a>
                    <a href="https://github.com/changhaonan/STAR-no-prior">STAR-non-prior</a>.
                  </p>
                  <p>In 2024, I was research intern at ByteDance foundation seeds, working on combining video-difffusion-model to long-horizon manipulation tasks.</p>
                  <p>In 2023, I was a research intern at MERL, working on contact-rich robot manipulation, colloborating with Siddarth Jain. Our <a
                      href="https://www.merl.com/publications/docs/TR2024-137.pdf">InsertOne</a> was accepted by
                    IROS2024.</p>
                  <p>In 2022, I was an applied scientist intern at Amazon Lab126, working on SLAM system for Astro robot.</p>
                  <p>Previously, I received Robotics M.S. and Mechanical Engineering M.S. from
                    University of Michigan, Ann Arbor, advised by
                    Prof. Chad Jenkins. I received my B.S. in Mechanical Engineering and Mathematics from Tsinghua University, Beijing, where I worked Prof. Chuxiong Hu.</p>
                  <p style="text-align:center">
                    <a href="mailto:chnme40cs@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/HaonanChang_CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=en&user=fb-yr1YAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://x.com/haonan_chang">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://github.com/changhaonan">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/HaonanChang.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/HaonanChang.jpg"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I have a broad interest in many different aspects of robotics, my research works covers perception, planning and control of robotics. Some papers are <span
                      class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="a3vlm_stop()" onmouseover="a3vlm_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='a3vlm_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/corl2024/a3_vlm.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/corl2024/a3_vlm.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function a3vlm_start() {
                      document.getElementById('a3vlm_image').style.opacity = "1";
                    }

                    function a3vlm_stop() {
                      document.getElementById('a3vlm_image').style.opacity = "0";
                    }
                    a3vlm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/changhaonan/A3VLM">
                    <span class="papertitle">A3VLM: Actionable Articulation-Aware Vision Language Model
                    </span>
                  </a>
                  <br>
                  <a href="https://siyuanhuang95.github.io/">Siyuan Huang</a>*,
                  <strong>Haonan Chang</strong>*,
                  <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
                  Yimeng Zhu,
                  <a href="https://zsdonghao.github.io/">Hao Dong</a>,
                  <a href="https://gaopengcuhk.github.io/">Peng Gao</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>,
                  <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                  <br>
                  <em>CoRL</em>, 2024 &nbsp
                  <br>
                  <a href="https://github.com/changhaonan/A3VLM">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2406.07549">arXiv</a>
                  <p></p>
                  <p>
                    We proposed an Articulation-aware Vision Language Model that is able to located the task-related articulation structure and affordance based on language task description.
                  </p>
                </td>
              </tr>

              <tr onmouseout="vkt_stop()" onmouseover="vkt_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='vkt_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/corl2024/vkt.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/corl2024/vkt.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function vkt_start() {
                      document.getElementById('vkt_image').style.opacity = "1";
                    }

                    function vkt_stop() {
                      document.getElementById('vkt_image').style.opacity = "0";
                    }
                    vkt_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://mlzxy.github.io/visual-kinetic-chain/">
                    <span class="papertitle">Scaling Manipulation Learning with Visual Kinematic Chain Prediction
                    </span>
                  </a>
                  <br>
                  <a href="https://mlzxy.github.io/">Xinyu Zhang</a>,
                  <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>,
                  <br>
                  <em>CoRL</em>, 2024 &nbsp
                  <br>
                  <a href="https://mlzxy.github.io/visual-kinetic-chain/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2406.07837">arXiv</a>
                  <p></p>
                  <p>
                    We proposed unified representation, i.e. Visual Kinematic Chain, to model different robotics tasks into a unified reprsentation for scalable training.
                  </p>
                </td>
              </tr>

              <!-- iros2024 -->
              <tr onmouseout="lgmcts_stop()" onmouseover="lgmcts_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='lgmcts_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iros2024/lgmcts.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iros2024/lgmcts.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function lgmcts_start() {
                      document.getElementById('lgmcts_image').style.opacity = "1";
                    }

                    function lgmcts_stop() {
                      document.getElementById('lgmcts_image').style.opacity = "0";
                    }
                    lgmcts_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://lgmcts.github.io/">
                    <span class="papertitle">LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement
                    </span>
                  </a>
                  <br>
                  <strong>Haonan Chang</strong>,
                  <a href="https://gaokai15.github.io/">Kai Gao</a>,
                  Yimeng Zhu,
                  <a href="https://kowndinya2000.github.io/">Kowndinya Boyalakuntla</a>,
                  <a href="https://www.linkedin.com/in/alex-lee-54a20421a/">Alex Lee</a>,
                  <a href="https://baichuan05.github.io/">Baichuan Huang</a>,
                  <a href="https://www.cs.rutgers.edu/people/directory.php?type=grad&netid=hu33">Harish Udhaya Kumar</a>,
                  <a href="https://arc-l.github.io/group.html">Jingjin Yu</a>,
                  <a href="https://ericjing.com/">Eric Jing</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <br>
                  <em>IROS</em>, 2024 &nbsp
                  <br>
                  <a href="https://lgmcts.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2406.07549">arXiv</a>
                  <p></p>
                  <p>
                    We combined LLM with Monte-Carlo Tree Search planner to solve exectuable semantic object rearrangement tasks.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it
                    includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a
                      href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Haonan Chang</title>

  <meta name="author" content="Haonan Chang">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Haonan Chang
                  </p>
                  <p>I'm a fifth-year robotics Ph.D. student at <b>Rutgers University</b> in New Brunswick, advised by Prof. Abdeslam Boularias. I am currently working on combining the latest
                    generative models, such as LLMs, VLMs, and VDMs, with robot manipulation to achieve general robotic manipulation abilities.
                  </p>
                  <p>
                    At Rutgers I've worked on LLM/VLM Drive Manipulation: <a href="https://github.com/changhaonan/A3VLM">A3VLM</a>, <a href="https://lgmcts.github.io/">LGMCTS</a>; LLM-driven Scene
                    understanding: <a href="https://github.com/changhaonan/OVSG">OVSG</a>; Dynamic Scene Reconstruction: <a href="https://github.com/changhaonan/Mono-STAR-demo">Mono-STAR</a>
                    <a href="https://github.com/changhaonan/STAR-no-prior">STAR-non-prior</a>.
                  </p>
                  <p>In 2024, I was a research intern at <b>ByteDance Foundation Seeds</b>, working on combining video diffusion models with long-horizon manipulation tasks.</p>
                  <p>In 2023, I was a research intern at <b>MERL</b>, working on contact-rich robot manipulation in collaboration with Dr. Siddarth Jain. Our paper, <a
                      href="https://www.merl.com/publications/docs/TR2024-137.pdf">InsertOne</a>, was accepted to IROS 2024.</p>
                  <p>In 2022, I was an Applied Scientist intern at <b>Amazon Lab126</b>, working on the SLAM system for the Astro robot.</p>
                  <p>Previously, I received an M.S. in Robotics and an M.S. in Mechanical Engineering from the <b>University of Michigan</b>, Ann Arbor, advised by Prof. Chad Jenkins. I received my
                    B.S. in Mechanical Engineering and Mathematics from <b>Tsinghua University</b>, Beijing, where I worked with Prof. Chuxiong Hu.</p>
                  <p style="text-align:center">
                    <a href="mailto:chnme40cs@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/HaonanChang_CV.pdf">CV</a> &nbsp;/&nbsp;
                    <a href="https://scholar.google.com/citations?hl=en&user=fb-yr1YAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <a href="https://x.com/haonan_chang">Twitter</a> &nbsp;/&nbsp;
                    <a href="https://github.com/changhaonan">Github</a> &nbsp;/&nbsp;
                    <a href="https://www.linkedin.com/in/haonan-chang-b0a38a130/">LinkedIn</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/HaonanChang.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/HaonanChang.jpg"
                      class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    I have a broad interest in various aspects of robotics, and my research covers perception, planning, and control of robots. Some papers are <span
                      class="highlight">highlighted</span>.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="arp_stop()" onmouseover="arp_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='arp_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iclr2025/arp.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iclr2025/arp.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function arp_start() {
                      document.getElementById('arp_image').style.opacity = "1";
                    }

                    function arp_stop() {
                      document.getElementById('arp_image').style.opacity = "0";
                    }
                    arp_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/mlzxy/arp">
                    <span class="papertitle">Autoregressive Action Sequence Learning for Robotic Manipulation
                    </span>
                  </a>
                  <br>
                  <a href="https://mlzxy.github.io/">Xinyu Zhang</a>,
                  <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://liamschramm.com/">Liam Schramm</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <br>
                  <!-- <em>In Submission</em>, 2024 &nbsp -->
                  <em>In Submission</em>, 2024 &nbsp <font color="red"><strong>(SOTA on RLBench)</strong></font>
                  <br>
                  <a href="https://github.com/mlzxy/arp">github</a>
                  /
                  <a href="https://arxiv.org/abs/2410.03132">arXiv</a>
                  <p></p>
                  <p>
                    We propose the Chunking Causal Transformer (CCT), which extends the next-single-token prediction of causal transformers to support multi-token prediction in a single pass. We
                    evaluate ARP across diverse robotic manipulation environments, including Push-T, ALOHA, and RLBench, and show that it outperforms the state-of-the-art methods in all tested
                    environments, while being more efficient in computation and parameter sizes.
                  </p>
                </td>
              </tr>

              <tr onmouseout="uniaff_stop()" onmouseover="uniaff_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='uniaff_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/icra2025/uniaff.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/icra2025/uniaff.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function uniaff_start() {
                      document.getElementById('uniaff_image').style.opacity = "1";
                    }

                    function uniaff_stop() {
                      document.getElementById('uniaff_image').style.opacity = "0";
                    }
                    uniaff_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://sites.google.com/view/uni-aff">
                    <span class="papertitle">UniAff: A Unified Representation of Affordances for Tool Usage and Articulation with Vision-Language Models
                    </span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=hOxT8QUAAAAJ&hl=zh-CN"> Qiaojun Yu</a>*,
                  <a href="https://siyuanhuang95.github.io/">Siyuan Huang</a>*,
                  Xibin Yuan, Zhengkai Jiang, Ce Hao, Xin Li,
                  <strong>Haonan Chang</strong>, Junbo Wang, Liu Liu,
                  <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>,
                  <a href="https://gaopengcuhk.github.io/">Peng Gao</a>,
                  <a href="https://scholar.google.com/citations?user=QZVQEWAAAAAJ&hl=en">Cewu Lu</a>
                  <br>
                  <em>In Submission</em>, 2024 &nbsp
                  <br>
                  <a href="https://sites.google.com/view/uni-aff">project page</a>
                  /
                  <a href="https://huggingface.co/papers/2409.20551">huggingface</a>
                  <p></p>
                  <p>
                    Previous studies on robotic manipulation are based on a limited understanding of the underlying 3D motion constraints and affordances. To address these challenges, we propose a
                    comprehensive paradigm, termed UniAff, that integrates 3D object-centric manipulation and task understanding in a unified formulation.
                  </p>
                </td>
              </tr>

              <tr onmouseout="a3vlm_stop()" onmouseover="a3vlm_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='a3vlm_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/corl2024/a3_vlm.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/corl2024/a3_vlm.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function a3vlm_start() {
                      document.getElementById('a3vlm_image').style.opacity = "1";
                    }

                    function a3vlm_stop() {
                      document.getElementById('a3vlm_image').style.opacity = "0";
                    }
                    a3vlm_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/changhaonan/A3VLM">
                    <span class="papertitle">A3VLM: Actionable Articulation-Aware Vision Language Model
                    </span>
                  </a>
                  <br>
                  <a href="https://siyuanhuang95.github.io/">Siyuan Huang</a>*,
                  <strong>Haonan Chang</strong>*,
                  <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
                  Yimeng Zhu,
                  <a href="https://zsdonghao.github.io/">Hao Dong</a>,
                  <a href="https://gaopengcuhk.github.io/">Peng Gao</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>,
                  <a href="https://www.ee.cuhk.edu.hk/~hsli/">Hongsheng Li</a>
                  <br>
                  <em>CoRL</em>, 2024 &nbsp
                  <br>
                  <a href="https://github.com/changhaonan/A3VLM">github</a>
                  /
                  <a href="https://arxiv.org/abs/2406.07549">arXiv</a>
                  <p></p>
                  <p>
                    We propose an Articulation-aware Vision Language Model that is able to located the task-related articulation structure and affordance based on language task description.
                  </p>
                </td>
              </tr>

              <tr onmouseout="vkt_stop()" onmouseover="vkt_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='vkt_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/corl2024/vkt.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/corl2024/vkt.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function vkt_start() {
                      document.getElementById('vkt_image').style.opacity = "1";
                    }

                    function vkt_stop() {
                      document.getElementById('vkt_image').style.opacity = "0";
                    }
                    vkt_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://mlzxy.github.io/visual-kinetic-chain/">
                    <span class="papertitle">Scaling Manipulation Learning with Visual Kinematic Chain Prediction
                    </span>
                  </a>
                  <br>
                  <a href="https://mlzxy.github.io/">Xinyu Zhang</a>,
                  <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>,
                  <br>
                  <em>CoRL</em>, 2024 &nbsp
                  <br>
                  <a href="https://mlzxy.github.io/visual-kinetic-chain/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2406.07837">arXiv</a>
                  /
                  <a href="https://github.com/mlzxy/visual-kinetic-chain">github</a>
                  <p></p>
                  <p>
                    We propose unified representation, i.e. Visual Kinematic Chain, to model different robotics tasks into a unified reprsentation for scalable training.
                  </p>
                </td>
              </tr>

              <!-- iros2024 -->
              <tr onmouseout="lgmcts_stop()" onmouseover="lgmcts_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='lgmcts_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iros2024/lgmcts.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iros2024/lgmcts.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function lgmcts_start() {
                      document.getElementById('lgmcts_image').style.opacity = "1";
                    }

                    function lgmcts_stop() {
                      document.getElementById('lgmcts_image').style.opacity = "0";
                    }
                    lgmcts_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://lgmcts.github.io/">
                    <span class="papertitle">LGMCTS: Language-Guided Monte-Carlo Tree Search for Executable Semantic Object Rearrangement
                    </span>
                  </a>
                  <br>
                  <strong>Haonan Chang</strong>,
                  <a href="https://gaokai15.github.io/">Kai Gao</a>,
                  Yimeng Zhu,
                  <a href="https://kowndinya2000.github.io/">Kowndinya Boyalakuntla</a>,
                  <a href="https://www.linkedin.com/in/alex-lee-54a20421a/">Alex Lee</a>,
                  <a href="https://baichuan05.github.io/">Baichuan Huang</a>,
                  <a href="https://www.cs.rutgers.edu/people/directory.php?type=grad&netid=hu33">Harish Udhaya Kumar</a>,
                  <a href="https://arc-l.github.io/group.html">Jingjin Yu</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <br>
                  <!-- <em>IROS</em>, 2024 &nbsp -->
                  <em>IROS</em>, 2024 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://lgmcts.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2406.07549">arXiv</a> /
                  <a href="https://github.com/changhaonan/LGMCTS-D">github</a>
                  <p></p>
                  <p>
                    We combined LLM with Monte-Carlo Tree Search planner to solve exectuable semantic object rearrangement tasks.
                  </p>
                </td>
              </tr>

              <tr onmouseout="insert1_stop()" onmouseover="insert1_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='insert1_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iros2024/insert1.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iros2024/insert1.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function insert1_start() {
                      document.getElementById('insert1_image').style.opacity = "1";
                    }

                    function insert1_stop() {
                      document.getElementById('insert1_image').style.opacity = "0";
                    }
                    insert1_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://www.merl.com/publications/docs/TR2024-137.pdf">
                    <span class="papertitle">Insert-One: One-Shot Robust Visual-Force Servoing for
                      Novel Object Insertion with 6-DoF Tracking
                    </span>
                  </a>
                  <br>
                  <strong>Haonan Chang</strong>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <a href="https://www.merl.com/people/sjain">Siddarth Jain</a>
                  <br>
                  <em>IROS</em>, 2024 &nbsp
                  <br>
                  <a href="https://www.merl.com/publications/docs/TR2024-137.pdf">arXiv</a>
                  <p></p>
                  <p>
                    We propose a two-stage, visual servoing + forcing servoing algorithm for insertion task on novel objects.
                  </p>
                </td>
              </tr>

              <tr onmouseout="dap_stop()" onmouseover="dap_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='dap_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iros2024/dap.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iros2024/dap.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function dap_start() {
                      document.getElementById('dap_image').style.opacity = "1";
                    }

                    function dap_stop() {
                      document.getElementById('dap_image').style.opacity = "0";
                    }
                    dap_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2409.00499">
                    <span class="papertitle">DAP: Diffusion-based Affordance Prediction for Multi-modality Storage
                    </span>
                  </a>
                  <br>
                  <strong>Haonan Chang</strong>,
                  <a href="https://kowndinya2000.github.io/">Kowndinya Boyalakuntla</a>,
                  <a href="https://jaysparrow.github.io/">Yuhan Liu</a>,
                  <a href="https://mlzxy.github.io/">Xinyu Zhang</a>,
                  <a href="https://liamschramm.com/">Liam Schramm</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <br>
                  <em>IROS</em>, 2024 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2409.00499">arXiv</a> /
                  <a href="https://github.com/changhaonan/DPS">github</a>
                  <p></p>
                  <p>
                    We propose diffusion-based affordance prediction architecture to locate the interactable region within a multi-modality storage problem.
                  </p>
                </td>
              </tr>

              <!-- corl23 -->
              <tr onmouseout=" ovsg_stop()" onmouseover="ovsg_start()" bgcolor="#ffffd0">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ovsg_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/corl2023/ovsg.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/corl2023/ovsg.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function ovsg_start() {
                      document.getElementById('ovsg_image').style.opacity = "1";
                    }

                    function ovsg_stop() {
                      document.getElementById('ovsg_image').style.opacity = "0";
                    }
                    ovsg_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ovsg-l.github.io/">
                    <span class="papertitle">Context-Aware Entity Grounding with Open-Vocabulary 3D Scene Graphs
                    </span>
                  </a>
                  <br>
                  <strong>Haonan Chang</strong>,
                  <a href="https://kowndinya2000.github.io/">Kowndinya Boyalakuntla</a>,
                  <a href="https://scholar.google.com/citations?user=pDH5AcsAAAAJ&hl=en">Shiyang Lu</a>,
                  <a href="https://www.linkedin.com/in/bill-cai/">Siwei Cai</a>,
                  <a href="https://ericjing.com/">Eric Jing</a>,
                  <a href="https://linkedin.com/in/keskarshreesh">Shreesh Keskar</a>,
                  <a href="https://scholar.google.com/citations?user=wujqvGYAAAAJ&hl=en">Shijie Geng</a>,
                  <a href="https://github.com/adeeb10abbas">Adeeb Abbas</a>,
                  <a href="https://lfzhou917.github.io/">Lifeng Zhou</a>,
                  <a href="https://robotics.cs.rutgers.edu/pracsys/members/kostas-bekris/">Kostas Bekris</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <br>
                  <em>CoRL</em>, 2023 &nbsp
                  <br>
                  <a href="https://ovsg-l.github.io/">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2309.15940">arXiv</a> /
                  <a href="https://github.com/changhaonan/OVSG">github</a>
                  <p></p>
                  <p>
                    We present an Open-Vocabulary 3D Scene Graph (OVSG), a formal framework for grounding a variety of entities, such as object instances, agents, and regions, with free-form
                    text-based queries.
                  </p>
                </td>
              </tr>

              <tr onmouseout="ovir_stop()" onmouseover="ovir_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ovir_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/corl2023/ovir3d.png" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/corl2023/ovir3d.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function ovir_start() {
                      document.getElementById('ovir_image').style.opacity = "1";
                    }

                    function ovir_stop() {
                      document.getElementById('ovir_image').style.opacity = "0";
                    }
                    ovir_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/shiyoung77/OVIR-3D">
                    <span class="papertitle">OVIR-3D: Open-Vocabulary 3D Instance Retrieval Without Training on 3D Data
                    </span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=pDH5AcsAAAAJ&hl=en">Shiyang Lu</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://ericjing.com/">Eric Jing</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>,
                  <a href="https://robotics.cs.rutgers.edu/pracsys/members/kostas-bekris/">Kostas Bekris</a>
                  <br>
                  <em>CoRL</em>, 2023 &nbsp
                  <br>
                  <a href="https://github.com/shiyoung77/OVIR-3D">project page</a>
                  /
                  <a href="https://arxiv.org/abs/2311.02873">arXiv</a>
                  <p></p>
                  <p>
                    We propose OVIR-3D, a straightforward yet effective method
                    for open-vocabulary 3D object instance retrieval without using any 3D data for
                    training.
                  </p>
                </td>
              </tr>

              <!-- iros22 -->
              <tr onmouseout="mono_star_stop()" onmouseover="mono_star_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='mono_star_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/icra2023/mono_star.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/icra2023/mono_star.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function mono_star_start() {
                      document.getElementById('mono_star_image').style.opacity = "1";
                    }

                    function mono_star_stop() {
                      document.getElementById('mono_star_image').style.opacity = "0";
                    }
                    mono_star_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/changhaonan/Mono-STAR-demo">
                    <span class="papertitle">Mono-STAR: Mono-camera Scene-level Tracking and Reconstruction
                    </span>
                  </a>
                  <br>
                  <strong>Haonan Chang</strong>,
                  <a href="https://www.linkedin.com/in/dhruvmetha/">Dhruv Metha</a>,
                  <a href="https://scholar.google.com/citations?user=wujqvGYAAAAJ&hl=en">Shijie Geng</a>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <br>
                  <em>IROS</em>, 2022 &nbsp
                  <br>
                  <a href="https://github.com/changhaonan/Mono-STAR-demo">github</a>
                  /
                  <a href="https://arxiv.org/abs/2301.13244">arXiv</a>
                  <p></p>
                  <p>
                    We present Mono-STAR, the first real-time 3D reconstruction system that simultaneously supports semantic fusion, fast motion tracking, non-rigid object deformation, and topological
                    change under a unified framework..
                  </p>
                </td>
              </tr>

              <!-- iros22 -->
              <tr onmouseout="star_stop()" onmouseover="star_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='star_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iros2022/star.gif" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iros2022/star.gif' width="160">
                  </div>
                  <script type="text/javascript">
                    function star_start() {
                      document.getElementById('star_image').style.opacity = "1";
                    }

                    function star_stop() {
                      document.getElementById('star_image').style.opacity = "0";
                    }
                    star_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://github.com/changhaonan/STAR-no-prior">
                    <span class="papertitle">Scene-level Tracking and Reconstruction without Object Priors
                    </span>
                  </a>
                  <br>
                  <strong>Haonan Chang</strong>,
                  <a href="https://scholar.google.com/citations?user=8AF3RCsAAAAJ&hl=en">Abdeslam Boularias</a>
                  <br>
                  <em>IROS</em>, 2022 &nbsp
                  <br>
                  <a href="https://github.com/changhaonan/STAR-no-prior">github</a>
                  /
                  <a href="https://arxiv.org/abs/2210.03815">arXiv</a>
                  <p></p>
                  <p>
                    We present the first real-time system capable of tracking and reconstructing, individually, every visible object in a given scene, without any form of prior on the rigidness of the
                    objects, texture existence, or object category.
                  </p>
                </td>
              </tr>

              <!-- iros19 -->
              <tr onmouseout="geo_stop()" onmouseover="geo_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='geo_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ral/geo_fusion.png" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/ral/geo_fusion.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function geo_start() {
                      document.getElementById('geo_image').style.opacity = "1";
                    }

                    function geo_stop() {
                      document.getElementById('geo_image').style.opacity = "0";
                    }
                    geo_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2003.12610">
                    <span class="papertitle">Geofusion: Geometric consistency informed scene estimation in dense clutter
                    </span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=3NagJCUAAAAJ&hl=en">Zhiqiang Sui</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://scholar.google.com/citations?user=6dhIxEgAAAAJ&hl=en">Ning Xu</a>,
                  <a href="https://web.eecs.umich.edu/~ocj/">Chad Jenkins</a>
                  <br>
                  <em>RAL</em>, 2020 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2003.12610">arXiv</a>
                  <p></p>
                  <p>
                    propose GeoFusion, a SLAM-based scene estimation method for building an object-level semantic map in dense
                    clutter.
                  </p>
                </td>
              </tr>

              <!-- iros19 -->
              <tr onmouseout="glass_stop()" onmouseover="glass_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='glass_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iros2019/grasp_loc.png" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iros2019/grasp_loc.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function glass_start() {
                      document.getElementById('glass_image').style.opacity = "1";
                    }

                    function glass_stop() {
                      document.getElementById('glass_image').style.opacity = "0";
                    }
                    glass_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2003.12610">
                    <span class="papertitle">glassfusion: glassmetric consistency informed scene estimation in dense clutter
                    </span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=LWKGD_kAAAAJ&hl=en">Zheming Zhou</a>,
                  <a href="https://www.tianyangpan.com/">Tianyang Pan</a>,
                  <a href="https://www.linkedin.com/in/shiyu-wu-713709261/">Shiyu Wu</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://web.eecs.umich.edu/~ocj/">Chad Jenkins</a>
                  <br>
                  <em>IROS</em>, 2019 &nbsp
                  <br>
                  <a href="https://arxiv.org/abs/2003.12610">arXiv</a>
                  <p></p>
                  <p>
                    Transparent objects are prevalent across many environments of interest for dexterous robotic manipulation. GlassLoc classifies graspable locations in space informed by a Depth
                    Likelihood Volume (DLV) descriptor.
                  </p>
                </td>
              </tr>

              <!-- ieee -->
              <tr onmouseout="ieee_stop()" onmouseover="ieee_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='ieee_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/ieee/gru.png" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/ieee/gru.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function ieee_start() {
                      document.getElementById('ieee_image').style.opacity = "1";
                    }

                    function ieee_stop() {
                      document.getElementById('ieee_image').style.opacity = "0";
                    }
                    ieee_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/document/9005235">
                    <span class="papertitle">Deep GRU neural network prediction and feedforward compensation for precision multiaxis motion control systems
                    </span>
                  </a>
                  <br>
                  <a href="https://www.me.tsinghua.edu.cn/en/info/1049/1890.htm#">Chuxiong Hu</a>,
                  <a href="https://scholar.google.com/citations?user=aDEoKUAAAAAJ&hl=en">Tiansheng Ou</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://www.me.tsinghua.edu.cn/en/info/1080/1192.htm">Yu Zhu</a>,
                  <a href="https://scholar.google.com.hk/citations?user=wNLK-GkAAAAJ&hl=zh-CN">Limin Zhu</a>
                  <br>
                  <em>IEEE/ASME Transactions on Mechatronics</em>, 2020 &nbsp
                  <br>
                  <a href="https://ieeexplore.ieee.org/document/9005235">IEEE</a>
                  <p></p>
                  <p>
                    We propose a gated recurrent unit (GRU) neural network prediction and compensation (NNC) strategy for precision multiaxis motion control systems with contouring performance
                    orientation.
                  </p>
                </td>
              </tr>

              <!-- iv -->
              <tr onmouseout="iv_stop()" onmouseover="iv_start()" bgcolor="#ffffff">
                <td style="padding:20px;width:25%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='iv_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/iv/iv.png" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/iv/iv.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function iv_start() {
                      document.getElementById('iv_image').style.opacity = "1";
                    }

                    function iv_stop() {
                      document.getElementById('iv_image').style.opacity = "0";
                    }
                    iv_stop()
                  </script>
                </td>
                <td style="padding:20px;width:75%;vertical-align:middle">
                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813861">
                    <span class="papertitle">Toward modularization of neural network autonomous driving policy using parallel attribute networks
                    </span>
                  </a>
                  <br>
                  <a href="https://scholar.google.com/citations?user=mIlaYj0AAAAJ&hl=en">Zhuo Xu</a>,
                  <strong>Haonan Chang</strong>,
                  <a href="https://chentangmark.github.io/">Chen Tang</a>,
                  <a href="https://www.ri.cmu.edu/ri-faculty/changliu-liu/">Changliu Liu</a>,
                  <br>
                  <em>IEEE Intelligent Vehicles Symposium</em>, 2019 &nbsp
                  <br>
                  <a href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8813861">arXiv</a>
                  <p></p>
                  <p>
                    We propose to modularize the
                    complicated driving policies in terms of the driving attributes,
                    and present the parallel attribute networks (PAN), which can
                    learn to fullfill the requirements of the attributes in the driving
                    tasks separately, and later assemble their knowledge together.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Feel free to steal this website's <a href="https://github.com/jonbarron/jonbarron_website">source code</a>. <strong>Do not</strong> scrape the HTML from this page itself, as it
                    includes analytics tags that you do not want on your own website &mdash; use the github code instead. Also, consider using <a href="https://leonidk.com/">Leonid Keselman</a>'s <a
                      href="https://github.com/leonidk/new_website">Jekyll fork</a> of this page.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>